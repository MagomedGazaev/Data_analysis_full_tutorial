{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-12T10:36:13.977312Z",
     "start_time": "2023-11-12T10:36:13.066688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py:33: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @jit\n"
     ]
    }
   ],
   "source": [
    "#Условие:\n",
    "# Input_size = 300 \n",
    "# Activation Function = Sigmoid -> исходя из этого условия, мы можем утверждать, что наша задача сводится к построению логистической регрессии, реализовывающую бинарную классификацию \n",
    "\n",
    "\n",
    "# Логистическая регрессия состоит из линейной регрессии (линейного преобразования) + функции активации\n",
    "# На нашем примере: синтез 300 линейных преобразований и одной функции активации (sigmoid), вносящую нелинейное преобразование (чтобы моделька выявила сложные паттерны/зависимости и обучалась на них)\n",
    "\n",
    "\n",
    "# Создаём модель ;-)\n",
    "\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import timeit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, learning_rate=0.01): #input_dim - число входных признаков, при вызове обозначим 300, как сказано в условии\n",
    "        self.weights = np.random.rand(input_dim) #300 раз генерирует случайные числа из равномерного распределения в диапазоне от 0 до 1, тем самым имитируя инициализацию весами для каждого входа\n",
    "        self.bias = np.random.rand() #аналогично делает для смещений, имитируя инициализацию\n",
    "        self.learning_rate = learning_rate #коэффициент \"небрежности\" при обновлении весов на основе градиента, чем меньше - тем плавнее мы будем обновлять веса\n",
    "\n",
    "    def sigmoid(self, z): # создаём нашу функцию активации\n",
    "        return 1 / (1 + np.exp(-z)) #её формула \n",
    "    \n",
    "    \n",
    "    def predict_dot(self, X): \n",
    "        z = np.dot(X, self.weights) + self.bias # реализуем линейное преобразование\n",
    "        return self.sigmoid(z) # посылаем результат линейного преобразования в функцию активации, чтобы осуществить предсказание вероятности принадлежности к классу \n",
    "    \n",
    "    \n",
    "    #версия 2, ручное умножение матриц + ускорение с jit\n",
    "    @jit\n",
    "    def predict_jit(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):  # Итерация по всем строкам в X\n",
    "            z = 0\n",
    "            for j in range(X.shape[1]): # Итерация по всем элементам в строке\n",
    "                z += X[i, j] * self.weights[j]  # Покоординатное умножение и суммирование\n",
    "            z += self.bias\n",
    "            y_pred[i] = self.sigmoid(z)  # Применение сигмоидной функции\n",
    "        return y_pred\n",
    "        \n",
    "\n",
    "    def loss(self, y_true, y_pred): #приниманием истинное и предсказанное \n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) #возвращаем потерю в результате нашего предсказание\n",
    "\n",
    "    def gradient_descent(self, X, y_true, y_pred):\n",
    "    # Ниже мы будем имитировать градиетный спуск, опишу зачем: \n",
    "    \n",
    "    # Мы получаем значения потерь и должны задаться следующим вопросом: \n",
    "    # «Какова механика изменения функции, в результате изменения её аргумента и чем она характеризуется?»\n",
    "    # На языке математики: «Найди скорость приращения функции, при приращении её аргумента»\n",
    "    # На языке практики это: «Какова механика изменения потерь, в результате изменения весов?»\n",
    "    \n",
    "    # Ответить на данный вопрос можно только после вычисления производной (при помощи Backpropagation)\n",
    "    # Результат которой является синтезом двух компонент (то она чем характеризуется)\n",
    "    \n",
    "    # Компонента 1 - Направление изменения функции (её секущая) ~ tg a\n",
    "    # Если tg a > 0 => функция растет / Если tg a < 0 => функция убывает\n",
    "    \n",
    "    # Компонента 2 - Скорость изменения функции в моменте (касательная)\n",
    "\n",
    "        m = len(y_true) # Давай Получаем количество обучающих примеров\n",
    "    \n",
    "        # Давай вычислим градиенты (производные) потерь по весам (dW) и смещению (dB)\n",
    "        dw = np.dot(X.T, (y_pred - y_true)) / m # Градиент по весам\n",
    "        db = np.sum(y_pred - y_true) / m # Градиент по смещению\n",
    "    \n",
    "        # Давай опишем как будем обновлять параметры модельки, на основе проделанных действий\n",
    "        self.weights -= self.learning_rate * dw # Обновляем веса (с учетом информации о градиенте)\n",
    "        self.bias -= self.learning_rate * db # Обновляем смещения (с учетом информации о градиенте)\n",
    "        \n",
    "    \n",
    "    def gradient_descent_jit(self, X, y_true, y_pred): #покоординатное умножение элементов матрицы \"руками\"\n",
    "        m = len(y_true)\n",
    "        \n",
    "        dw = np.zeros_like(self.weights)\n",
    "        db = 0\n",
    "        \n",
    "        #вычисляем градиенты\n",
    "        for i in range(m):\n",
    "            for j in range(X.shape[1]):\n",
    "                dw[j] += (y_pred[i] - y_true[i]) * X[i, j]\n",
    "            db += (y_pred[i] - y_true[i])\n",
    "        \n",
    "        dw /= m\n",
    "        db /= m\n",
    "        \n",
    "        #Обновляем веса и смещение\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "                \n",
    "\n",
    "    def fit_dot(self, X, y, epochs=500): #описываем алгоритм тренировочного цикла в виде вызываемой функции\n",
    "        for epoch in range(epochs): # 500 эпох = 500 раз проходимся по всему датасету, в процессе обучения модели\n",
    "            y_pred = self.predict_dot(X) # Вычисление предсказаний модели на текущем наборе данных\n",
    "            y_pred_labels = (y_pred > 0.5).astype(int) #преобразуем вероятности в метки посредством округления\n",
    "            self.gradient_descent(X, y, y_pred) # Запуск обновление весов и смещения модели с использованием градиентного спуска\n",
    "            if epoch % 10 == 0: # Вывод информации о процессе обучения каждую эпоху\n",
    "                print(f\"Epoch {epoch}: Loss = {self.loss(y, y_pred)}\")\n",
    "            \n",
    "        # Оценим качество модели на сгенерированном синтетическом датасете:\n",
    "        # Вывод результатов\n",
    "        report = classification_report(y, y_pred_labels)\n",
    "        conf_matrix = confusion_matrix(y, y_pred_labels)\n",
    "            \n",
    "        return report, conf_matrix\n",
    "    \n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def fit_jit(self, X, y, epochs=500): #описываем алгоритм тренировочного цикла в виде вызываемой функции\n",
    "        for epoch in range(epochs): # 500 эпох = 500 раз проходимся по всему датасету, в процессе обучения модели\n",
    "            y_pred = self.predict_jit(X) # Вычисление предсказаний модели на текущем наборе данных\n",
    "            y_pred_labels = (y_pred > 0.5).astype(int) #преобразуем вероятности в метки посредством округления\n",
    "            self.gradient_descent_jit(X, y, y_pred) # Обновление весов и смещения модели с использованием градиентного спуска\n",
    "            if epoch % 10 == 0: # Вывод информации о процессе обучения каждую эпоху\n",
    "                print(f\"Epoch {epoch}: Loss = {self.loss(y, y_pred)}\")\n",
    "                \n",
    "        # Оценим качество модели на сгенерированном синтетическом датасете:\n",
    "        # Вывод результатов\n",
    "        report = classification_report(y, y_pred_labels)\n",
    "        conf_matrix = confusion_matrix(y, y_pred_labels)\n",
    "            \n",
    "        return report, conf_matrix\n",
    "        \n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 4.256383888301372\n",
      "Epoch 10: Loss = 4.189481939116288\n",
      "Epoch 20: Loss = 4.123704538196315\n",
      "Epoch 30: Loss = 4.059033249043813\n",
      "Epoch 40: Loss = 3.995448379464272\n",
      "Epoch 50: Loss = 3.93292917639433\n",
      "Epoch 60: Loss = 3.8714547994997814\n",
      "Epoch 70: Loss = 3.8110050015955546\n",
      "Epoch 80: Loss = 3.7515608017159208\n",
      "Epoch 90: Loss = 3.693104937935982\n",
      "Epoch 100: Loss = 3.6356223055034014\n",
      "Epoch 110: Loss = 3.5790999795701772\n",
      "Epoch 120: Loss = 3.5235272477747634\n",
      "Epoch 130: Loss = 3.468895170370713\n",
      "Epoch 140: Loss = 3.4151957749422257\n",
      "Epoch 150: Loss = 3.362420929215047\n",
      "Epoch 160: Loss = 3.3105609674934384\n",
      "Epoch 170: Loss = 3.2596036315350507\n",
      "Epoch 180: Loss = 3.209533759835291\n",
      "Epoch 190: Loss = 3.1603338536446652\n",
      "Epoch 200: Loss = 3.111985314687546\n",
      "Epoch 210: Loss = 3.0644698579061087\n",
      "Epoch 220: Loss = 3.0177707255513417\n",
      "Epoch 230: Loss = 2.9718733848738417\n",
      "Epoch 240: Loss = 2.9267658390128464\n",
      "Epoch 250: Loss = 2.8824385510425725\n",
      "Epoch 260: Loss = 2.8388841477381708\n",
      "Epoch 270: Loss = 2.796096875122337\n",
      "Epoch 280: Loss = 2.7540718723273856\n",
      "Epoch 290: Loss = 2.7128043363049414\n",
      "Epoch 300: Loss = 2.6722886197634397\n",
      "Epoch 310: Loss = 2.6325174785359877\n",
      "Epoch 320: Loss = 2.5934816279586013\n",
      "Epoch 330: Loss = 2.5551696751791177\n",
      "Epoch 340: Loss = 2.5175684032138927\n",
      "Epoch 350: Loss = 2.4806632862233156\n",
      "Epoch 360: Loss = 2.4444390867618613\n",
      "Epoch 370: Loss = 2.408880388509211\n",
      "Epoch 380: Loss = 2.3739720061416225\n",
      "Epoch 390: Loss = 2.339699228591856\n",
      "Epoch 400: Loss = 2.306047908932318\n",
      "Epoch 410: Loss = 2.273004388545957\n",
      "Epoch 420: Loss = 2.2405553170136083\n",
      "Epoch 430: Loss = 2.2086874361548876\n",
      "Epoch 440: Loss = 2.177387392959876\n",
      "Epoch 450: Loss = 2.1466417000069806\n",
      "Epoch 460: Loss = 2.1164368658555106\n",
      "Epoch 470: Loss = 2.086759688996462\n",
      "Epoch 480: Loss = 2.057597650644567\n",
      "Epoch 490: Loss = 2.0289393108521963\n",
      "Report (dot):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63       393\n",
      "           1       0.65      0.67      0.66       407\n",
      "\n",
      "    accuracy                           0.65       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.65      0.64       800\n",
      "\n",
      "Confusion Matrix (dot):\n",
      "[[243 150]\n",
      " [134 273]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py:33: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"predict_jit\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py (33)\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 33:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py:33: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"predict_jit\" failed type inference due to: Cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 36:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/magomedgazaev/Deep Learning/Pycharm_DL_Projects/lib/python3.11/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"predict_jit\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 33:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/Users/magomedgazaev/Deep Learning/Pycharm_DL_Projects/lib/python3.11/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 33:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "/var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py:33: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"predict_jit\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py (36)\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 36:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/Users/magomedgazaev/Deep Learning/Pycharm_DL_Projects/lib/python3.11/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"predict_jit\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 36:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/Users/magomedgazaev/Deep Learning/Pycharm_DL_Projects/lib/python3.11/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"../../../../var/folders/gv/nkdtfy695vx8nny1jj5gqygm0000gn/T/ipykernel_1491/1656287518.py\", line 36:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.000774627996841\n",
      "Epoch 10: Loss = 1.9730951475924048\n",
      "Epoch 20: Loss = 1.9458940351766336\n",
      "Epoch 30: Loss = 1.9191659682647326\n",
      "Epoch 40: Loss = 1.8929068994766567\n",
      "Epoch 50: Loss = 1.8671137377804052\n",
      "Epoch 60: Loss = 1.8417839794957696\n",
      "Epoch 70: Loss = 1.8169153384030279\n",
      "Epoch 80: Loss = 1.7925054162087497\n",
      "Epoch 90: Loss = 1.768551438535489\n",
      "Epoch 100: Loss = 1.7450500794397226\n",
      "Epoch 110: Loss = 1.7219973707352687\n",
      "Epoch 120: Loss = 1.6993886838354673\n",
      "Epoch 130: Loss = 1.6772187632188615\n",
      "Epoch 140: Loss = 1.6554817880210897\n",
      "Epoch 150: Loss = 1.6341714360974715\n",
      "Epoch 160: Loss = 1.613280938950527\n",
      "Epoch 170: Loss = 1.5928031157574185\n",
      "Epoch 180: Loss = 1.5727303862807311\n",
      "Epoch 190: Loss = 1.5530547703021853\n",
      "Epoch 200: Loss = 1.5337678861644826\n",
      "Epoch 210: Loss = 1.5148609615142776\n",
      "Epoch 220: Loss = 1.4963248679568728\n",
      "Epoch 230: Loss = 1.4781501867857145\n",
      "Epoch 240: Loss = 1.4603273033493536\n",
      "Epoch 250: Loss = 1.4428465209937469\n",
      "Epoch 260: Loss = 1.4256981846894203\n",
      "Epoch 270: Loss = 1.4088727997199577\n",
      "Epoch 280: Loss = 1.3923611362896657\n",
      "Epoch 290: Loss = 1.3761543122874462\n",
      "Epoch 300: Loss = 1.3602438523960791\n",
      "Epoch 310: Loss = 1.3446217226372172\n",
      "Epoch 320: Loss = 1.3292803439986807\n",
      "Epoch 330: Loss = 1.3142125879671185\n",
      "Epoch 340: Loss = 1.299411758248365\n",
      "Epoch 350: Loss = 1.2848715625375007\n",
      "Epoch 360: Loss = 1.2705860771314996\n",
      "Epoch 370: Loss = 1.256549707887768\n",
      "Epoch 380: Loss = 1.2427571495633096\n",
      "Epoch 390: Loss = 1.2292033451082096\n",
      "Epoch 400: Loss = 1.215883447089708\n",
      "Epoch 410: Loss = 1.202792781938255\n",
      "Epoch 420: Loss = 1.189926818081308\n",
      "Epoch 430: Loss = 1.1772811389708422\n",
      "Epoch 440: Loss = 1.1648514208520402\n",
      "Epoch 450: Loss = 1.1526334160252094\n",
      "Epoch 460: Loss = 1.14062294117145\n",
      "Epoch 470: Loss = 1.1288158707364893\n",
      "Epoch 480: Loss = 1.1172081344297649\n",
      "Epoch 490: Loss = 1.105795718806522\n",
      "Report (jit):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       393\n",
      "           1       0.74      0.78      0.76       407\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.75      0.75      0.75       800\n",
      "weighted avg       0.75      0.75      0.75       800\n",
      "\n",
      "Confusion Matrix (jit):\n",
      "[[282 111]\n",
      " [ 88 319]]\n",
      "Время выполнения с np.dot: 0.00027233303990215063 секунд\n",
      "Время выполнения с numba.jit: 0.473667333018966 секунд\n"
     ]
    }
   ],
   "source": [
    "# Давай используем, но прежде нужно отправить подготовленный датасет в нашу модельку\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Создаем синтетический датасет\n",
    "X, y = make_classification(n_samples=1000, n_features=300, n_classes=2, random_state=42)\n",
    "\n",
    "# Разделяем данные на тренировочный и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Приводим наши данные к единому стандарту\n",
    "# чтобы каждый признак (или столбец данных) имел среднее значение (математическое ожидание) 0 \n",
    "# А стандартное отклонение было равно 1. \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "# Создание экземпляра модели\n",
    "model = LogisticRegression(input_dim=300) \n",
    "\n",
    "#Обучаем модельки и выводим метрики оценки качества\n",
    "report_dot, conf_matrix_dot = model.fit_dot(X_train_scaled, y_train) #вызов обучения по dot\n",
    "print(\"Report (dot):\")\n",
    "print(report_dot)\n",
    "print(\"Confusion Matrix (dot):\")\n",
    "print(conf_matrix_dot)\n",
    "\n",
    "report_jit, conf_matrix_jit = model.fit_jit(X_train_scaled, y_train) #вызов обучения по jit\n",
    "print(\"Report (jit):\")\n",
    "print(report_jit)\n",
    "print(\"Confusion Matrix (jit):\")\n",
    "print(conf_matrix_jit)\n",
    "\n",
    "#Вызываем \n",
    "prediction_using_dot = model.predict_dot(X_train_scaled)  # Вызов метода predict_dot\n",
    "prediction_using_jit = model.predict_jit(X_train_scaled)  # Вызов метода predict_jit\n",
    "\n",
    "\n",
    "# Функции для замера времени\n",
    "def time_predict_dot():\n",
    "    model.predict_dot(X_train_scaled) # Вызов метода predict_dot\n",
    "\n",
    "def time_predict_jit():\n",
    "    model.predict_jit(X_train_scaled) # Вызов метода predict_jit\n",
    "\n",
    "# Измеряем время\n",
    "time_dot = timeit.timeit(time_predict_dot, number=10)\n",
    "time_jit = timeit.timeit(time_predict_jit, number=10)\n",
    "\n",
    "# Операция выполняется 10 раз для получения более надежной оценки времени выполнения кода. \n",
    "# При многократном выполнении кода усредняется время выполнения, что помогает уменьшить влияние случайных задержек, \n",
    "# которые могут возникать в системе (например, другие процессы, работающие на компьютере, влияние кэширования и т.д.).\n",
    "\n",
    "print(f\"Время выполнения с np.dot: {time_dot} секунд\")\n",
    "print(f\"Время выполнения с numba.jit: {time_jit} секунд\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T10:37:18.031675Z",
     "start_time": "2023-11-12T10:36:17.175243Z"
    }
   },
   "id": "f21e5e61622af809"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Время выполнения с np.dot: 0.00027233303990215063 секунд\n",
    "#Время выполнения с numba.jit: 0.473667333018966 секунд"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "551b7d7584277233"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
